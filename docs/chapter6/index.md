---
title: "ç¬¬6ç« : å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé›†"
chapter: chapter6
layout: book
---

# ç¬¬6ç« : å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé›†

## ã¯ã˜ã‚ã«ï¼šå®Ÿéš›ã«å‹•ãã‚‚ã®ã‚’ä½œã‚ã†ï¼

ã“ã‚Œã¾ã§å­¦ã‚“ã çŸ¥è­˜ã‚’ä½¿ã£ã¦ã€å®Ÿéš›ã«å‹•ãã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œã‚Šã¾ã™ã€‚
æœ€åˆã¯é›£ã—ãæ„Ÿã˜ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€ã‚³ãƒ”ãƒšã—ãªãŒã‚‰å‹•ã‹ã—ã¦ã€å°‘ã—ãšã¤ç†è§£ã—ã¦ã„ãã¾ã—ã‚‡ã†ã€‚

### ğŸ“‹ ã“ã®ç« ã§ä½œã‚‹ã‚‚ã®
1. **Webã‚µã‚¤ãƒˆå…¬é–‹ç’°å¢ƒ**ï¼ˆLAMPï¼‰
2. **Dockerã‚³ãƒ³ãƒ†ãƒŠç’°å¢ƒ**
3. **ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ **
4. **ãƒ­ã‚°è§£æã‚·ã‚¹ãƒ†ãƒ **
5. **è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤ç’°å¢ƒ**
6. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–**
7. **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ **

## 6.1 LAMPç’°å¢ƒæ§‹ç¯‰

### LAMPã£ã¦ä½•ï¼Ÿ

**LAMP**ã¯4ã¤ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã®é ­æ–‡å­—ã§ã™ï¼š
- **L**inux: OSï¼ˆã‚‚ã†ä½¿ã£ã¦ã¾ã™ï¼ï¼‰
- **A**pache: Webã‚µãƒ¼ãƒãƒ¼ï¼ˆãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã‚’å…¬é–‹ï¼‰
- **M**ySQL: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼‰
- **P**HP: ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªï¼ˆå‹•çš„ãªãƒšãƒ¼ã‚¸ã‚’ä½œã‚‹ï¼‰

ã“ã‚Œã‚‰ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã¨ã€WordPressã®ã‚ˆã†ãªWebã‚µã‚¤ãƒˆãŒä½œã‚Œã¾ã™ã€‚

### ğŸ¯ å®Œæˆã‚¤ãƒ¡ãƒ¼ã‚¸
1. ãƒ–ãƒ©ã‚¦ã‚¶ã§ `http://localhost` ã«ã‚¢ã‚¯ã‚»ã‚¹
2. WordPressã®ç®¡ç†ç”»é¢ãŒè¡¨ç¤ºã•ã‚Œã‚‹
3. ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’æŠ•ç¨¿ã§ãã‚‹

### Apache Webã‚µãƒ¼ãƒãƒ¼ï¼ˆã‚¹ãƒ†ãƒƒãƒ—1/4ï¼‰

```bash
# ã¾ãšã¯ã‚·ã‚¹ãƒ†ãƒ ã‚’æœ€æ–°ã«
sudo apt update

# Apacheã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆWebã‚µãƒ¼ãƒãƒ¼ï¼‰
sudo apt install apache2 -y

# èµ·å‹•ã¨è‡ªå‹•èµ·å‹•è¨­å®š
sudo systemctl start apache2    # ä»Šã™ãèµ·å‹•
sudo systemctl enable apache2   # PCèµ·å‹•æ™‚ã«è‡ªå‹•èµ·å‹•

# å‹•ä½œç¢ºèª
sudo systemctl status apache2
# ç·‘è‰²ã§ã€Œactive (running)ã€ã¨è¡¨ç¤ºã•ã‚Œã‚Œã°OKï¼
```

ğŸ‰ **ç¢ºèªæ–¹æ³•**ï¼š
Windowsã®ãƒ–ãƒ©ã‚¦ã‚¶ã§ `http://localhost` ã‚’é–‹ã
â†’ ã€ŒApache2 Ubuntu Default Pageã€ãŒè¡¨ç¤ºã•ã‚Œã‚Œã°æˆåŠŸï¼

Apacheè¨­å®šã®é‡è¦é …ç›®ï¼š
```apache
# ã‚µãƒ¼ãƒãƒ¼åè¨­å®šï¼ˆè­¦å‘ŠæŠ‘åˆ¶ï¼‰
ServerName localhost

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
DirectoryIndex index.html index.php

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ˜ãƒƒãƒ€ãƒ¼
ServerTokens Prod
ServerSignature Off
```

### MySQL ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹

```bash
# MySQLã‚µãƒ¼ãƒãƒ¼ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
sudo apt install mysql-server -y

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š
sudo mysql_secure_installation

# è³ªå•ã¸ã®æ¨å¥¨å›ç­”ï¼š
# - VALIDATE PASSWORD component: Y
# - Password validation level: 1 (MEDIUM)
# - Remove anonymous users: Y
# - Disallow root login remotely: Y
# - Remove test database: Y
# - Reload privilege tables: Y

# MySQLãƒ­ã‚°ã‚¤ãƒ³
sudo mysql
```

ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆï¼š
```sql
-- WordPressç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆ
CREATE DATABASE wordpress CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;

-- ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆã¨æ¨©é™ä»˜ä¸
CREATE USER 'wpuser'@'localhost' IDENTIFIED BY 'StrongPassword123!';
GRANT ALL PRIVILEGES ON wordpress.* TO 'wpuser'@'localhost';
FLUSH PRIVILEGES;

-- ç¢ºèª
SHOW DATABASES;
SELECT user, host FROM mysql.user;
EXIT;
```

### PHP ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨è¨­å®š

```bash
# PHP ã¨å¿…è¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
sudo apt install php libapache2-mod-php php-mysql php-curl php-gd php-mbstring php-xml php-zip -y

# ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
php -v

# PHPè¨­å®š
sudo nano /etc/php/8.3/apache2/php.ini
```

PHPæ¨å¥¨è¨­å®šï¼š
```ini
; ãƒ¡ãƒ¢ãƒªåˆ¶é™
memory_limit = 256M

; ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚µã‚¤ã‚º
upload_max_filesize = 64M
post_max_size = 64M

; ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³
date.timezone = Asia/Tokyo

; ã‚¨ãƒ©ãƒ¼è¡¨ç¤ºï¼ˆæœ¬ç•ªç’°å¢ƒã§ã¯ Offï¼‰
display_errors = Off
log_errors = On
error_log = /var/log/php_errors.log
```

### WordPress ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# WordPress ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
cd /tmp
wget https://wordpress.org/latest.tar.gz
tar -xzvf latest.tar.gz

# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ«ãƒ¼ãƒˆã¸é…ç½®
sudo cp -R wordpress /var/www/html/
sudo chown -R www-data:www-data /var/www/html/wordpress
sudo chmod -R 755 /var/www/html/wordpress

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
cd /var/www/html/wordpress
sudo cp wp-config-sample.php wp-config.php
sudo nano wp-config.php
```

wp-config.php è¨­å®šï¼š
```php
// ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š
define( 'DB_NAME', 'wordpress' );
define( 'DB_USER', 'wpuser' );
define( 'DB_PASSWORD', 'StrongPassword123!' );
define( 'DB_HOST', 'localhost' );
define( 'DB_CHARSET', 'utf8mb4' );

// ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚­ãƒ¼ï¼ˆhttps://api.wordpress.org/secret-key/1.1/salt/ ã‹ã‚‰å–å¾—ï¼‰
// å„ã‚­ãƒ¼ã‚’ç”Ÿæˆã•ã‚ŒãŸå€¤ã«ç½®ãæ›ãˆ
```

### Apacheä»®æƒ³ãƒ›ã‚¹ãƒˆè¨­å®š

```bash
# ä»®æƒ³ãƒ›ã‚¹ãƒˆä½œæˆ
sudo nano /etc/apache2/sites-available/wordpress.conf
```

```apache
<VirtualHost *:80>
    ServerName wordpress.local
    DocumentRoot /var/www/html/wordpress
    
    <Directory /var/www/html/wordpress>
        Options FollowSymLinks
        AllowOverride All
        Require all granted
    </Directory>
    
    ErrorLog ${APACHE_LOG_DIR}/wordpress_error.log
    CustomLog ${APACHE_LOG_DIR}/wordpress_access.log combined
</VirtualHost>
```

```bash
# ã‚µã‚¤ãƒˆæœ‰åŠ¹åŒ–
sudo a2ensite wordpress.conf
sudo a2enmod rewrite
sudo systemctl reload apache2

# hosts ãƒ•ã‚¡ã‚¤ãƒ«ç·¨é›†ï¼ˆWindowså´ï¼‰
# C:\Windows\System32\drivers\etc\hosts ã«è¿½åŠ 
# 127.0.0.1 wordpress.local
```

## 6.2 Dockerç’°å¢ƒæ§‹ç¯‰

### Docker ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# å‰æãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
sudo apt update
sudo apt install apt-transport-https ca-certificates curl software-properties-common -y

# Docker GPGã‚­ãƒ¼è¿½åŠ 
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg

# Dockerãƒªãƒã‚¸ãƒˆãƒªè¿½åŠ 
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# Docker ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
sudo apt update
sudo apt install docker-ce docker-ce-cli containerd.io docker-compose-plugin -y

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’dockerã‚°ãƒ«ãƒ¼ãƒ—ã«è¿½åŠ 
sudo usermod -aG docker $USER
newgrp docker

# å‹•ä½œç¢ºèª
docker --version
docker run hello-world
```

### Docker Compose ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

```bash
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
mkdir -p ~/docker-projects/webapp
cd ~/docker-projects/webapp

# docker-compose.yml ä½œæˆ
nano docker-compose.yml
```

docker-compose.ymlï¼š
```yaml
version: '3.8'

services:
  web:
    image: nginx:alpine
    container_name: webapp-nginx
    ports:
      - "8080:80"
    volumes:
      - ./html:/usr/share/nginx/html
      - ./nginx.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - php
    networks:
      - webapp-network

  php:
    image: php:8.2-fpm-alpine
    container_name: webapp-php
    volumes:
      - ./html:/var/www/html
      - ./php.ini:/usr/local/etc/php/php.ini
    networks:
      - webapp-network

  db:
    image: mysql:8.0
    container_name: webapp-mysql
    environment:
      MYSQL_ROOT_PASSWORD: rootpass123
      MYSQL_DATABASE: webapp
      MYSQL_USER: webapp_user
      MYSQL_PASSWORD: userpass123
    volumes:
      - db_data:/var/lib/mysql
    ports:
      - "3306:3306"
    networks:
      - webapp-network

  phpmyadmin:
    image: phpmyadmin:latest
    container_name: webapp-phpmyadmin
    environment:
      PMA_HOST: db
      PMA_PORT: 3306
    ports:
      - "8081:80"
    depends_on:
      - db
    networks:
      - webapp-network

volumes:
  db_data:

networks:
  webapp-network:
    driver: bridge
```

Nginxè¨­å®šï¼ˆnginx.confï¼‰ï¼š
```nginx
server {
    listen 80;
    server_name localhost;
    root /usr/share/nginx/html;
    index index.php index.html;

    location / {
        try_files $uri $uri/ /index.php?$query_string;
    }

    location ~ \.php$ {
        fastcgi_pass php:9000;
        fastcgi_index index.php;
        fastcgi_param SCRIPT_FILENAME /var/www/html$fastcgi_script_name;
        include fastcgi_params;
    }
}
```

ãƒ†ã‚¹ãƒˆãƒšãƒ¼ã‚¸ä½œæˆï¼š
```bash
# HTMLãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
mkdir html

# PHPãƒ†ã‚¹ãƒˆãƒšãƒ¼ã‚¸
cat << 'PHP' > html/index.php
<?php
phpinfo();
?>
PHP

# èµ·å‹•
docker compose up -d

# ç¢ºèª
docker compose ps
curl http://localhost:8080
```

### Dockerfile ã«ã‚ˆã‚‹ã‚«ã‚¹ã‚¿ãƒ ã‚¤ãƒ¡ãƒ¼ã‚¸

```bash
# Node.jsã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ä¾‹
mkdir -p ~/docker-projects/nodeapp
cd ~/docker-projects/nodeapp

# Dockerfileä½œæˆ
nano Dockerfile
```

Dockerfileï¼š
```dockerfile
# ãƒ™ãƒ¼ã‚¹ã‚¤ãƒ¡ãƒ¼ã‚¸
FROM node:18-alpine

# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
WORKDIR /app

# ä¾å­˜é–¢ä¿‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼
COPY package*.json ./

# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
RUN npm ci --only=production

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ”ãƒ¼
COPY . .

# ãƒãƒ¼ãƒˆå…¬é–‹
EXPOSE 3000

# å®Ÿè¡Œãƒ¦ãƒ¼ã‚¶ãƒ¼
USER node

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•
CMD ["node", "index.js"]
```

ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆï¼š
```javascript
// index.js
const http = require('http');
const os = require('os');

const server = http.createServer((req, res) => {
    res.writeHead(200, { 'Content-Type': 'text/plain' });
    res.end(`Hello from Docker!\nHostname: ${os.hostname()}\n`);
});

server.listen(3000, () => {
    console.log('Server running on port 3000');
});
```

```json
// package.json
{
  "name": "nodeapp",
  "version": "1.0.0",
  "description": "Simple Node.js app",
  "main": "index.js",
  "scripts": {
    "start": "node index.js"
  }
}
```

ãƒ“ãƒ«ãƒ‰ã¨å®Ÿè¡Œï¼š
```bash
# ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒ“ãƒ«ãƒ‰
docker build -t nodeapp:v1 .

# ã‚³ãƒ³ãƒ†ãƒŠå®Ÿè¡Œ
docker run -d -p 3000:3000 --name myapp nodeapp:v1

# ãƒ­ã‚°ç¢ºèª
docker logs myapp
```

## 6.3 ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰

### ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
#!/bin/bash
# system_monitor.sh - ç·åˆã‚·ã‚¹ãƒ†ãƒ ç›£è¦–

# è¨­å®š
MONITOR_DIR="/var/log/monitoring"
ALERT_EMAIL="admin@example.com"
SLACK_WEBHOOK="https://hooks.slack.com/services/YOUR/WEBHOOK/URL"

# ã—ãã„å€¤
THRESHOLD_CPU=80
THRESHOLD_MEM=90
THRESHOLD_DISK=85
THRESHOLD_LOAD=4.0

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
sudo mkdir -p $MONITOR_DIR

# ãƒ­ã‚°è¨˜éŒ²é–¢æ•°
log_metric() {
    local metric=$1
    local value=$2
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    echo "$timestamp,$metric,$value" >> "$MONITOR_DIR/metrics.csv"
}

# ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡é–¢æ•°
send_alert() {
    local level=$1
    local message=$2
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    # ãƒ­ã‚°è¨˜éŒ²
    echo "[$timestamp] [$level] $message" >> "$MONITOR_DIR/alerts.log"
    
    # Slacké€šçŸ¥ï¼ˆWebhookURLè¨­å®šæ™‚ï¼‰
    if [ -n "$SLACK_WEBHOOK" ]; then
        curl -X POST -H 'Content-type: application/json' \
            --data "{\"text\":\"[$level] $message\"}" \
            "$SLACK_WEBHOOK"
    fi
    
    # ãƒ¡ãƒ¼ãƒ«é€šçŸ¥ï¼ˆmailè¨­å®šæ™‚ï¼‰
    # echo "$message" | mail -s "[$level] System Alert" $ALERT_EMAIL
}

# CPUç›£è¦–
check_cpu() {
    local cpu_usage=$(top -bn1 | grep "Cpu(s)" | awk '{print int($2)}')
    log_metric "cpu_usage" "$cpu_usage"
    
    if [ $cpu_usage -gt $THRESHOLD_CPU ]; then
        send_alert "WARNING" "High CPU usage: ${cpu_usage}%"
        return 1
    fi
    return 0
}

# ãƒ¡ãƒ¢ãƒªç›£è¦–
check_memory() {
    local mem_info=$(free | grep Mem)
    local total=$(echo $mem_info | awk '{print $2}')
    local used=$(echo $mem_info | awk '{print $3}')
    local usage=$(echo "scale=0; $used * 100 / $total" | bc)
    
    log_metric "memory_usage" "$usage"
    
    if [ $usage -gt $THRESHOLD_MEM ]; then
        send_alert "WARNING" "High memory usage: ${usage}%"
        return 1
    fi
    return 0
}

# ãƒ‡ã‚£ã‚¹ã‚¯ç›£è¦–
check_disk() {
    local disk_usage=$(df -h / | tail -1 | awk '{print int($5)}')
    log_metric "disk_usage" "$disk_usage"
    
    if [ $disk_usage -gt $THRESHOLD_DISK ]; then
        send_alert "WARNING" "High disk usage: ${disk_usage}%"
        return 1
    fi
    return 0
}

# ãƒ­ãƒ¼ãƒ‰ã‚¢ãƒ™ãƒ¬ãƒ¼ã‚¸ç›£è¦–
check_load() {
    local load_1min=$(uptime | awk -F'load average:' '{print $2}' | awk '{print $1}' | tr -d ',')
    log_metric "load_average" "$load_1min"
    
    if (( $(echo "$load_1min > $THRESHOLD_LOAD" | bc -l) )); then
        send_alert "WARNING" "High load average: $load_1min"
        return 1
    fi
    return 0
}

# ã‚µãƒ¼ãƒ“ã‚¹ç›£è¦–
check_services() {
    local services=("nginx" "mysql" "ssh")
    
    for service in "${services[@]}"; do
        if ! systemctl is-active --quiet $service; then
            send_alert "ERROR" "Service $service is down"
            
            # è‡ªå‹•å†èµ·å‹•è©¦è¡Œ
            sudo systemctl restart $service
            sleep 5
            
            if systemctl is-active --quiet $service; then
                send_alert "INFO" "Service $service restarted successfully"
            else
                send_alert "CRITICAL" "Failed to restart $service"
            fi
        fi
    done
}

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
main() {
    check_cpu
    check_memory
    check_disk
    check_load
    check_services
}

# å®Ÿè¡Œ
main
```

### Prometheus + Grafana ç›£è¦–

```bash
# Prometheusã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
cd /tmp
wget https://github.com/prometheus/prometheus/releases/download/v2.45.0/prometheus-2.45.0.linux-amd64.tar.gz
tar -xzf prometheus-2.45.0.linux-amd64.tar.gz
sudo mv prometheus-2.45.0.linux-amd64 /opt/prometheus

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
sudo nano /opt/prometheus/prometheus.yml
```

prometheus.ymlï¼š
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
  
  - job_name: 'node'
    static_configs:
      - targets: ['localhost:9100']
```

Node Exporterã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼š
```bash
# Node Exporterï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ï¼‰
wget https://github.com/prometheus/node_exporter/releases/download/v1.6.0/node_exporter-1.6.0.linux-amd64.tar.gz
tar -xzf node_exporter-1.6.0.linux-amd64.tar.gz
sudo mv node_exporter-1.6.0.linux-amd64/node_exporter /usr/local/bin/

# systemdã‚µãƒ¼ãƒ“ã‚¹ä½œæˆ
sudo nano /etc/systemd/system/node_exporter.service
```

```ini
[Unit]
Description=Node Exporter
After=network.target

[Service]
Type=simple
User=nobody
Group=nogroup
ExecStart=/usr/local/bin/node_exporter

[Install]
WantedBy=multi-user.target
```

Grafanaè¨­å®šï¼š
```bash
# Grafanaã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
sudo apt-get install -y software-properties-common
sudo add-apt-repository "deb https://packages.grafana.com/oss/deb stable main"
wget -q -O - https://packages.grafana.com/gpg.key | sudo apt-key add -
sudo apt update
sudo apt install grafana -y

# èµ·å‹•
sudo systemctl start grafana-server
sudo systemctl enable grafana-server

# ã‚¢ã‚¯ã‚»ã‚¹: http://localhost:3000
# åˆæœŸãƒ­ã‚°ã‚¤ãƒ³: admin/admin
```

## 6.4 ãƒ­ã‚°åé›†ãƒ»è§£æã‚·ã‚¹ãƒ†ãƒ 

### ELKã‚¹ã‚¿ãƒƒã‚¯ç°¡æ˜“ç‰ˆ

```bash
# Elasticsearchï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒãƒ¼ãƒ‰ï¼‰
docker run -d \
  --name elasticsearch \
  -p 9200:9200 \
  -p 9300:9300 \
  -e "discovery.type=single-node" \
  -e "ES_JAVA_OPTS=-Xms512m -Xmx512m" \
  elasticsearch:7.17.10

# Kibana
docker run -d \
  --name kibana \
  -p 5601:5601 \
  --link elasticsearch \
  -e "ELASTICSEARCH_HOSTS=http://elasticsearch:9200" \
  kibana:7.17.10
```

### Fluentd ã«ã‚ˆã‚‹ãƒ­ã‚°åé›†

```bash
# Fluentd ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
curl -fsSL https://toolbelt.treasuredata.com/sh/install-ubuntu-noble-fluent-package-v5-lts.sh | sh

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
sudo nano /etc/fluent/fluent.conf
```

fluent.confï¼š
```xml
# Nginxã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°
<source>
  @type tail
  path /var/log/nginx/access.log
  pos_file /var/log/td-agent/nginx-access.pos
  tag nginx.access
  <parse>
    @type nginx
  </parse>
</source>

# ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°
<source>
  @type tail
  path /var/log/syslog
  pos_file /var/log/td-agent/syslog.pos
  tag system.syslog
  <parse>
    @type syslog
  </parse>
</source>

# Elasticsearchå‡ºåŠ›
<match **>
  @type elasticsearch
  host localhost
  port 9200
  logstash_format true
  logstash_prefix fluentd
  logstash_dateformat %Y%m%d
  include_tag_key true
  type_name access_log
  tag_key @log_name
  flush_interval 1s
</match>
```

### ãƒ­ã‚°è§£æã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
#!/bin/bash
# log_analyzer.sh - ã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°è§£æ

LOG_FILE="/var/log/nginx/access.log"
REPORT_DIR="/var/www/html/reports"
REPORT_FILE="$REPORT_DIR/report_$(date +%Y%m%d).html"

# ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
mkdir -p $REPORT_DIR

# HTMLé–‹å§‹
cat << 'HTML' > $REPORT_FILE
<!DOCTYPE html>
<html>
<head>
    <title>Access Log Report</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        table { border-collapse: collapse; width: 100%; margin: 20px 0; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #4CAF50; color: white; }
        h2 { color: #333; }
    </style>
</head>
<body>
    <h1>Nginx Access Log Analysis Report</h1>
    <p>Generated: $(date)</p>
HTML

# ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°
echo "<h2>Summary</h2>" >> $REPORT_FILE
echo "<p>Total Requests: $(wc -l < $LOG_FILE)</p>" >> $REPORT_FILE

# ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰åˆ†æ
echo "<h2>Status Codes</h2>" >> $REPORT_FILE
echo "<table><tr><th>Status Code</th><th>Count</th></tr>" >> $REPORT_FILE

awk '{print $9}' $LOG_FILE | sort | uniq -c | sort -rn | while read count code; do
    echo "<tr><td>$code</td><td>$count</td></tr>" >> $REPORT_FILE
done
echo "</table>" >> $REPORT_FILE

# ãƒˆãƒƒãƒ—10 IP
echo "<h2>Top 10 IP Addresses</h2>" >> $REPORT_FILE
echo "<table><tr><th>IP Address</th><th>Requests</th></tr>" >> $REPORT_FILE

awk '{print $1}' $LOG_FILE | sort | uniq -c | sort -rn | head -10 | while read count ip; do
    echo "<tr><td>$ip</td><td>$count</td></tr>" >> $REPORT_FILE
done
echo "</table>" >> $REPORT_FILE

# ãƒˆãƒƒãƒ—10 URL
echo "<h2>Top 10 URLs</h2>" >> $REPORT_FILE
echo "<table><tr><th>URL</th><th>Requests</th></tr>" >> $REPORT_FILE

awk '{print $7}' $LOG_FILE | sort | uniq -c | sort -rn | head -10 | while read count url; do
    echo "<tr><td>$url</td><td>$count</td></tr>" >> $REPORT_FILE
done
echo "</table>" >> $REPORT_FILE

# HTMLçµ‚äº†
echo "</body></html>" >> $REPORT_FILE

echo "Report generated: $REPORT_FILE"
```

## 6.5 CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰

### GitLab Runner ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# GitLab Runner ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
curl -L --output /usr/local/bin/gitlab-runner https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-linux-amd64
chmod +x /usr/local/bin/gitlab-runner

# ã‚µãƒ¼ãƒ“ã‚¹ç™»éŒ²
sudo gitlab-runner install --user=gitlab-runner --working-directory=/home/gitlab-runner
sudo gitlab-runner start

# Runnerç™»éŒ²
sudo gitlab-runner register
# å¯¾è©±å¼ã§è¨­å®šå…¥åŠ›
```

### .gitlab-ci.yml ä¾‹

```yaml
stages:
  - build
  - test
  - deploy

variables:
  APP_NAME: "myapp"
  DEPLOY_SERVER: "production.example.com"

# ãƒ“ãƒ«ãƒ‰ã‚¹ãƒ†ãƒ¼ã‚¸
build:
  stage: build
  image: node:18
  script:
    - npm ci
    - npm run build
  artifacts:
    paths:
      - dist/
    expire_in: 1 week

# ãƒ†ã‚¹ãƒˆã‚¹ãƒ†ãƒ¼ã‚¸
test:
  stage: test
  image: node:18
  script:
    - npm ci
    - npm test
    - npm run lint
  coverage: '/Coverage: \d+\.\d+%/'

# ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ãƒ†ãƒ¼ã‚¸
deploy:
  stage: deploy
  image: alpine:latest
  before_script:
    - apk add --no-cache openssh-client rsync
    - eval $(ssh-agent -s)
    - echo "$SSH_PRIVATE_KEY" | tr -d '\r' | ssh-add -
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh
  script:
    - rsync -avz --delete dist/ ${DEPLOY_USER}@${DEPLOY_SERVER}:/var/www/${APP_NAME}/
    - ssh ${DEPLOY_USER}@${DEPLOY_SERVER} "sudo systemctl restart ${APP_NAME}"
  only:
    - main
```

### Jenkins ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```groovy
// Jenkinsfile
pipeline {
    agent any
    
    environment {
        DOCKER_IMAGE = 'myapp'
        DOCKER_TAG = "${env.BUILD_NUMBER}"
    }
    
    stages {
        stage('Checkout') {
            steps {
                checkout scm
            }
        }
        
        stage('Build') {
            steps {
                sh 'docker build -t ${DOCKER_IMAGE}:${DOCKER_TAG} .'
            }
        }
        
        stage('Test') {
            steps {
                sh 'docker run --rm ${DOCKER_IMAGE}:${DOCKER_TAG} npm test'
            }
        }
        
        stage('Push') {
            when {
                branch 'main'
            }
            steps {
                withCredentials([usernamePassword(
                    credentialsId: 'docker-hub',
                    usernameVariable: 'DOCKER_USER',
                    passwordVariable: 'DOCKER_PASS'
                )]) {
                    sh 'echo $DOCKER_PASS | docker login -u $DOCKER_USER --password-stdin'
                    sh 'docker push ${DOCKER_IMAGE}:${DOCKER_TAG}'
                }
            }
        }
        
        stage('Deploy') {
            when {
                branch 'main'
            }
            steps {
                sshagent(['deploy-key']) {
                    sh '''
                        ssh user@server "docker pull ${DOCKER_IMAGE}:${DOCKER_TAG}"
                        ssh user@server "docker stop myapp || true"
                        ssh user@server "docker run -d --name myapp -p 80:3000 ${DOCKER_IMAGE}:${DOCKER_TAG}"
                    '''
                }
            }
        }
    }
    
    post {
        success {
            slackSend(
                color: 'good',
                message: "Deployment successful: ${env.JOB_NAME} - ${env.BUILD_NUMBER}"
            )
        }
        failure {
            slackSend(
                color: 'danger',
                message: "Deployment failed: ${env.JOB_NAME} - ${env.BUILD_NUMBER}"
            )
        }
    }
}
```

## 6.6 ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–

### ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«è¨­å®š

```bash
#!/bin/bash
# firewall_setup.sh - ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š

# UFWè¨­å®š
sudo ufw --force reset
sudo ufw default deny incoming
sudo ufw default allow outgoing

# å¿…è¦ãªãƒãƒ¼ãƒˆã®ã¿é–‹æ”¾
sudo ufw allow 22/tcp   # SSH
sudo ufw allow 80/tcp   # HTTP
sudo ufw allow 443/tcp  # HTTPS

# ç‰¹å®šIPã‹ã‚‰ã®SSHã®ã¿è¨±å¯ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
# sudo ufw allow from 192.168.1.100 to any port 22

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆãƒ–ãƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ¼ã‚¹å¯¾ç­–ï¼‰
sudo ufw limit ssh/tcp

# ãƒ­ã‚°æœ‰åŠ¹åŒ–
sudo ufw logging on

# ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«æœ‰åŠ¹åŒ–
sudo ufw --force enable

echo "Firewall configured successfully"
```

### Fail2banè¨­å®š

```bash
# Fail2banã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
sudo apt install fail2ban -y

# ã‚«ã‚¹ã‚¿ãƒ è¨­å®š
sudo nano /etc/fail2ban/jail.local
```

jail.localï¼š
```ini
[DEFAULT]
bantime = 3600
findtime = 600
maxretry = 5

[sshd]
enabled = true
port = ssh
logpath = %(sshd_log)s
maxretry = 3

[nginx-http-auth]
enabled = true
filter = nginx-http-auth
port = http,https
logpath = /var/log/nginx/error.log

[nginx-noscript]
enabled = true
port = http,https
filter = nginx-noscript
logpath = /var/log/nginx/access.log
maxretry = 6

[nginx-badbots]
enabled = true
port = http,https
filter = nginx-badbots
logpath = /var/log/nginx/access.log
maxretry = 2

[nginx-noproxy]
enabled = true
port = http,https
filter = nginx-noproxy
logpath = /var/log/nginx/error.log
maxretry = 2
```

### SSL/TLSè¨­å®šï¼ˆLet's Encryptï¼‰

```bash
# Certbotã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
sudo apt install certbot python3-certbot-nginx -y

# SSLè¨¼æ˜æ›¸å–å¾—ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³å¿…è¦ï¼‰
sudo certbot --nginx -d example.com -d www.example.com

# è‡ªå‹•æ›´æ–°è¨­å®š
sudo certbot renew --dry-run

# Cronè¨­å®š
echo "0 0,12 * * * root python3 -c 'import random; import time; time.sleep(random.random() * 3600)' && certbot renew -q" | sudo tee -a /etc/crontab > /dev/null
```

## 6.7 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ãƒªã‚¹ãƒˆã‚¢

### çµ±åˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ 

```bash
#!/bin/bash
# backup_system.sh - çµ±åˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

set -euo pipefail

# è¨­å®š
BACKUP_ROOT="/backup"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="$BACKUP_ROOT/$TIMESTAMP"
RETENTION_DAYS=30

# S3è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
S3_BUCKET="s3://my-backup-bucket"
AWS_PROFILE="default"

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¯¾è±¡
BACKUP_ITEMS=(
    "/etc"
    "/var/www"
    "/home"
)

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒªã‚¹ãƒˆ
DATABASES=(
    "wordpress"
    "webapp"
)

# ãƒ­ã‚°é–¢æ•°
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$BACKUP_ROOT/backup.log"
}

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
backup_directories() {
    log "Starting directory backup..."
    
    for item in "${BACKUP_ITEMS[@]}"; do
        if [ -d "$item" ]; then
            local backup_name=$(echo $item | tr '/' '_')
            tar -czf "$BACKUP_DIR/files${backup_name}.tar.gz" \
                --exclude='*.log' \
                --exclude='*.tmp' \
                "$item" 2>/dev/null || true
            log "Backed up: $item"
        fi
    done
}

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
backup_databases() {
    log "Starting database backup..."
    
    for db in "${DATABASES[@]}"; do
        mysqldump -u root \
            --single-transaction \
            --routines \
            --triggers \
            --events \
            "$db" | gzip > "$BACKUP_DIR/db_${db}.sql.gz"
        log "Backed up database: $db"
    done
}

# Dockerãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
backup_docker_volumes() {
    log "Starting Docker volume backup..."
    
    docker volume ls -q | while read volume; do
        docker run --rm \
            -v "$volume":/source:ro \
            -v "$BACKUP_DIR":/backup \
            alpine tar -czf "/backup/docker_${volume}.tar.gz" -C /source .
        log "Backed up Docker volume: $volume"
    done
}

# S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
upload_to_s3() {
    if command -v aws &> /dev/null; then
        log "Uploading to S3..."
        aws s3 sync "$BACKUP_DIR" "$S3_BUCKET/$TIMESTAMP/" \
            --profile "$AWS_PROFILE" \
            --storage-class GLACIER_IR
        log "Upload to S3 completed"
    fi
}

# å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤
cleanup_old_backups() {
    log "Cleaning up old backups..."
    
    find "$BACKUP_ROOT" -maxdepth 1 -type d -name "20*" \
        -mtime +$RETENTION_DAYS -exec rm -rf {} \;
    
    # S3ã‹ã‚‰ã‚‚å‰Šé™¤
    if command -v aws &> /dev/null; then
        aws s3 ls "$S3_BUCKET/" --profile "$AWS_PROFILE" | \
            awk '{print $2}' | \
            while read dir; do
                dir_date=$(echo $dir | cut -d'_' -f1)
                if [ $(date -d "$dir_date" +%s 2>/dev/null || echo 0) -lt \
                     $(date -d "$RETENTION_DAYS days ago" +%s) ]; then
                    aws s3 rm "$S3_BUCKET/$dir" --recursive --profile "$AWS_PROFILE"
                fi
            done
    fi
    
    log "Cleanup completed"
}

# ãƒªã‚¹ãƒˆã‚¢é–¢æ•°
restore_backup() {
    local backup_date=$1
    local restore_dir="/restore/$backup_date"
    
    log "Starting restore from $backup_date..."
    
    # S3ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå¿…è¦ãªå ´åˆï¼‰
    if [ ! -d "$BACKUP_ROOT/$backup_date" ]; then
        aws s3 sync "$S3_BUCKET/$backup_date/" "$BACKUP_ROOT/$backup_date/" \
            --profile "$AWS_PROFILE"
    fi
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚¢
    mkdir -p "$restore_dir"
    for file in "$BACKUP_ROOT/$backup_date"/files*.tar.gz; do
        tar -xzf "$file" -C "$restore_dir"
        log "Restored: $(basename $file)"
    done
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã‚¢
    for db_file in "$BACKUP_ROOT/$backup_date"/db_*.sql.gz; do
        local db_name=$(basename "$db_file" .sql.gz | cut -d'_' -f2)
        gunzip < "$db_file" | mysql -u root "$db_name"
        log "Restored database: $db_name"
    done
    
    log "Restore completed to $restore_dir"
}

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
main() {
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¢ãƒ¼ãƒ‰
    if [ "${1:-backup}" == "backup" ]; then
        log "=== Starting backup process ==="
        
        mkdir -p "$BACKUP_DIR"
        
        backup_directories
        backup_databases
        backup_docker_volumes
        upload_to_s3
        cleanup_old_backups
        
        log "=== Backup completed successfully ==="
        
    # ãƒªã‚¹ãƒˆã‚¢ãƒ¢ãƒ¼ãƒ‰
    elif [ "$1" == "restore" ]; then
        if [ -z "${2:-}" ]; then
            echo "Usage: $0 restore YYYYMMDD_HHMMSS"
            exit 1
        fi
        restore_backup "$2"
    fi
}

# å®Ÿè¡Œ
main "$@"
```

## 6.8 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
#!/bin/bash
# performance_tuning.sh - ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–

# ã‚«ãƒ¼ãƒãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
sudo tee /etc/sysctl.d/99-performance.conf << 'EOF'
# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æœ€é©åŒ–
net.core.somaxconn = 65535
net.ipv4.tcp_max_syn_backlog = 8192
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_fin_timeout = 30
net.ipv4.ip_local_port_range = 10000 65000

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿
fs.file-max = 2097152

# ãƒ¡ãƒ¢ãƒªç®¡ç†
vm.swappiness = 10
vm.dirty_ratio = 15
vm.dirty_background_ratio = 5
EOF

sudo sysctl -p /etc/sysctl.d/99-performance.conf

# Nginxæœ€é©åŒ–
sudo tee /etc/nginx/conf.d/performance.conf << 'EOF'
# ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹
worker_processes auto;
worker_rlimit_nofile 65535;

events {
    worker_connections 4096;
    use epoll;
    multi_accept on;
}

http {
    # ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º
    client_body_buffer_size 128k;
    client_max_body_size 10m;
    client_header_buffer_size 1k;
    large_client_header_buffers 4 8k;
    
    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
    client_body_timeout 12;
    client_header_timeout 12;
    keepalive_timeout 15;
    send_timeout 10;
    
    # Gzipåœ§ç¸®
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_comp_level 6;
    gzip_types text/plain text/css text/xml text/javascript 
               application/json application/javascript application/xml+rss 
               application/rss+xml application/atom+xml image/svg+xml 
               text/x-js text/x-cross-domain-policy application/x-font-ttf 
               application/x-font-opentype application/vnd.ms-fontobject 
               image/x-icon;
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    open_file_cache max=2000 inactive=20s;
    open_file_cache_valid 30s;
    open_file_cache_min_uses 2;
    open_file_cache_errors on;
}
EOF

# MySQLæœ€é©åŒ–
sudo tee -a /etc/mysql/mysql.conf.d/mysqld.cnf << 'EOF'

[mysqld]
# ãƒãƒƒãƒ•ã‚¡ãƒ—ãƒ¼ãƒ«ï¼ˆRAMã®70%ç¨‹åº¦ï¼‰
innodb_buffer_pool_size = 1G
innodb_log_file_size = 256M
innodb_flush_log_at_trx_commit = 2
innodb_flush_method = O_DIRECT

# æ¥ç¶šæ•°
max_connections = 500
max_connect_errors = 1000000

# ã‚¯ã‚¨ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥
query_cache_type = 1
query_cache_size = 128M
query_cache_limit = 2M

# ãã®ä»–
tmp_table_size = 64M
max_heap_table_size = 64M
thread_cache_size = 8
EOF

# ã‚µãƒ¼ãƒ“ã‚¹å†èµ·å‹•
sudo systemctl restart nginx
sudo systemctl restart mysql

echo "Performance tuning completed"
```

## ã¾ã¨ã‚

ç¬¬6ç« ã§å®Ÿè£…ã—ãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼š

1. **LAMPç’°å¢ƒ**: WordPresså‹•ä½œç’°å¢ƒã®å®Œå…¨æ§‹ç¯‰
2. **Dockerç’°å¢ƒ**: ã‚³ãƒ³ãƒ†ãƒŠåŒ–ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç®¡ç†
3. **ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ **: Prometheus/Grafanaã«ã‚ˆã‚‹å¯è¦–åŒ–
4. **ãƒ­ã‚°è§£æ**: ELKã‚¹ã‚¿ãƒƒã‚¯ã¨Fluentd
5. **CI/CD**: è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
6. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«ã¨SSL
7. **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**: çµ±åˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ 
8. **æœ€é©åŒ–**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

ã“ã‚Œã‚‰ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€å®Ÿéš›ã®é‹ç”¨ç’°å¢ƒã§ä½¿ç”¨ã§ãã‚‹å®Ÿè·µçš„ãªå†…å®¹ã§ã™ã€‚å„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’é †æ¬¡å®Ÿè£…ã™ã‚‹ã“ã¨ã§ã€ç·åˆçš„ãªã‚¤ãƒ³ãƒ•ãƒ©ç®¡ç†ã‚¹ã‚­ãƒ«ãŒèº«ã«ã¤ãã¾ã™ã€‚